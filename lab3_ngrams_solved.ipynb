{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "139758_lab3_ngrams_solved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEdmAZsqklfo"
      },
      "source": [
        "# Modele językowe - n-gramy \n",
        "\n",
        "---\n",
        "\n",
        "## 1. N-gramy słów w klasyfikacji\n",
        "Poniżej stworzono kod, który przeprowadza klasyfikację dokumentów należących do 4 kategorii. W odróżnieniu do poprzednich zajęć - tu zaproponowano klasyfikator SVC (algorytm SVM, popularna alternatywa dla NaiveBayes), która również świetnie się spisuje w problemach klasyfikacji tekstu.\n",
        "\n",
        "**<span style=\"color: red\">Zadanie 1a (0.5 punktu)</span>** Uruchom kod, przyjrzyj się wygenerowanym wynikom (a najlepiej zachowaj je gdzieś, będą potrzebne). <br/>\n",
        "\n",
        "\n",
        "\n",
        "Zapoznaj się z dokumentacją TfIdfVectorizer, odnajdź opcję uwzględnienia nie tylko pojedynczych słów jako cechy, ale także ich par i **zmodyfikuj poniższy kod tak, aby klasyfikacja uwzględniała zarówno pojedyncze słowa jak i pary (pozostaw parametr max_df=0.1 nienaruszony).** <span style=\"color: red\"> Zmodyfikuj linię 30.</span><br/> <br/>\n",
        "**<span style=\"color: red\">Zadanie 1b (0.5 punktu)</span>** Jak zmieniła się liczba cech po uwzględnieniu tych par? Czy coś się zmieniło w raporcie z klasyfikacji? Uzupełnij odpowiedzi na pytania w komórce poniżej kodu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO0jQYNKklfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1faa1ce-bc2f-447a-d071-68685343f84d"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_20newsgroups # zbiór danych zawarty w Sklearn, który zawiera dane z 20 grup newsowych\n",
        "import numpy as np\n",
        "\n",
        "# ------------------- WCZYTANIE DANYCH -----------\n",
        "\n",
        "np.random.seed(0) # ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
        "\n",
        "categories = ['sci.space', 'comp.graphics', 'talk.politics.misc', 'comp.sys.mac.hardware'] # kategorie do analizy\n",
        "\n",
        "train = fetch_20newsgroups(subset='train',\n",
        "                                   categories=categories,\n",
        "                                   shuffle=True,\n",
        "                                   random_state=42) # pobieramy zbiór uczący (na nim będziemy trenować) dla wybranych kategorii.\n",
        "    \n",
        "\n",
        "test = fetch_20newsgroups(subset='test',\n",
        "                                  categories=categories,\n",
        "                                  shuffle=True,\n",
        "                                  random_state=42) # pobieramy zbiór testowy (na nim będziemy testować) dla wybranych kategorii\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- STWORZENIE PIPELINE'U -----------\n",
        "    \n",
        "pipeline = Pipeline([             # stwórzmy pipeline surowy tekst -> TFIDF vectorizer -> klasyfikator \n",
        "    ('tfidf', TfidfVectorizer(max_df=0.1, ngram_range=(1, 2))),\n",
        "    ('clf', SVC(C=1.0, kernel='linear')),\n",
        "])\n",
        "\n",
        "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
        "\n",
        "pipeline.fit(train.data, train.target) # zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
        "\n",
        "\n",
        "print(\"W słowniku znajduje się {n} różnych cech\".format(\n",
        "    n=len(pipeline.named_steps['tfidf'].vocabulary_.keys())\n",
        "))\n",
        "\n",
        "# ------------------- OCENA KLASYFIKATORA -----------\n",
        "print(classification_report(test.target, pipeline.predict(test.data))) # testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary: precision, recall, f1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W słowniku znajduje się 287718 różnych cech\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.94      0.91       389\n",
            "           1       0.92      0.94      0.93       385\n",
            "           2       0.95      0.90      0.92       394\n",
            "           3       0.97      0.90      0.93       310\n",
            "\n",
            "    accuracy                           0.92      1478\n",
            "   macro avg       0.93      0.92      0.92      1478\n",
            "weighted avg       0.92      0.92      0.92      1478\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "'''# 1a BEZ MODYFIKACJI \\n W słowniku znajduje się 34774 różnych cech\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.88      0.93      0.90       389\n",
        "           1       0.91      0.93      0.92       385\n",
        "           2       0.94      0.91      0.92       394\n",
        "           3       0.96      0.90      0.93       310\n",
        "\n",
        "   accuracy                           0.92      1478\n",
        "   macro avg       0.92      0.92      0.92      1478\n",
        "weighted avg       0.92      0.92      0.92      1478''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34dY-20RJaW5",
        "outputId": "3ab06366-000c-477d-caf7-da6707a8bfcc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 1a BEZ MODYFIKACJI \n",
            " W słowniku znajduje się 34774 różnych cech\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.90       389\n",
            "           1       0.91      0.93      0.92       385\n",
            "           2       0.94      0.91      0.92       394\n",
            "           3       0.96      0.90      0.93       310\n",
            "\n",
            "   accuracy                           0.92      1478\n",
            "   macro avg       0.92      0.92      0.92      1478\n",
            "weighted avg       0.92      0.92      0.92      1478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''"
      ],
      "metadata": {
        "id": "Hn_eJmMqJNt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(''' 1b PO MODYFIKACJI \\n W słowniku znajduje się 287718 różnych cech\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.87      0.94      0.91       389\n",
        "           1       0.92      0.94      0.93       385\n",
        "           2       0.95      0.90      0.92       394\n",
        "           3       0.97      0.90      0.93       310\n",
        "\n",
        "    accuracy                           0.92      1478\n",
        "   macro avg       0.93      0.92      0.92      1478\n",
        "weighted avg       0.92      0.92      0.92      1478''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE5ieC5aKMY5",
        "outputId": "909a1e38-afb8-4f3c-d26a-5e6b3509a8a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1b PO MODYFIKACJI \n",
            " W słowniku znajduje się 287718 różnych cech\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.94      0.91       389\n",
            "           1       0.92      0.94      0.93       385\n",
            "           2       0.95      0.90      0.92       394\n",
            "           3       0.97      0.90      0.93       310\n",
            "\n",
            "    accuracy                           0.92      1478\n",
            "   macro avg       0.93      0.92      0.92      1478\n",
            "weighted avg       0.92      0.92      0.92      1478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "V8Zy2uT4klfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4ba538-8db2-431c-aee0-2f957645e70a"
      },
      "source": [
        "print('''1. O ile zwiększyła się liczba cech w klasyfikatorze? ODP: 287718 - 34774 = 252944\n",
        "2. Czy precyzja w którejkolwiek klasie wzrosła? w której/których? ODP: wzrost w 1,2,3, a spadek w 0\n",
        "3. Czy recall w którejkolwiek klasie wzrósł? w której/których? ODP: spadek w 0,1 wzrost w 2, bez zmian w 3''' )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. O ile zwiększyła się liczba cech w klasyfikatorze? ODP: 287718 - 34774 = 252944\n",
            "2. Czy precyzja w którejkolwiek klasie wzrosła? w której/których? ODP: wzrost w 1,2,3, a spadek w 0\n",
            "3. Czy recall w którejkolwiek klasie wzrósł? w której/których? ODP: spadek w 0,1 wzrost w 2, bez zmian w 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PX6uWjNklfy"
      },
      "source": [
        "## 2. N-gramy liter w klasyfikacji\n",
        "Poza n-gramami stworzonymi z następujących po sobie wyrazów - bardzo często używane są również n-gramy znakowe, stworzone z następujących po sobie liter. <br/><br/>\n",
        "Dla przykładu. wszystkie 3-gramy (trigramy) znakowe z napisu \"Hello world\" to: <br/>\n",
        "\"Hel\", \"ell\", \"llo\", \"lo \", \"o w\", \" wo\", \"wor\", \"orl\", \"rld\". <br/><br/>\n",
        "Do czego mogłaby być użyta taka reprezentacja tekstów? Okazuje się, że całkiem mocno pomaga to w rozwiązywaniiu problemu detekcji języka w którym został zapisany dokument, szczególnie w sytuacji, kiedy teksty są bardzo krótkie (np. tweety, smsy).\n",
        "<br/>\n",
        "Poniżej znajduje się szkielet klasyfikatora rozpoznającego język w którym zapisany jest dokument.\n",
        "Języków jest 6: polski, angielski, niemiecki, francuski, hiszpański i włoski.\n",
        "<br/>\n",
        "**<span style=\"color: red\">Zadanie 2 (1 punkt)</span>**: Przedstawiony klasyfikator jest znanym już z poprzednich przykładów kodem. Waszym zadaniem jest:\n",
        "<ol>\n",
        "    <li>Zapoznanie się dokumentacją Tf-Idf vectorizera, aby znaleźć funkcjonalność, która zamiast całych słów, stworzy cechy na podstawie liter i wykorzystanie tej funkcjonalności w kodzie</li>\n",
        "    <li>Ustawienie takiego zakresu n-gramów, aby zmaksymalizować uzyskany wynik (Oczekiwane 1.0 precyzji i recallu we wszystkich kategoriach przy pozostawieniu wartośi max_features = 300 elementów)</li>\n",
        "    <li>Poprawnie zaklasyfikuje krotki przykład zapisany w linii 43 (Bonjour przypisze do kategorii 'french').</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwJnn2vNLbUi",
        "outputId": "0e3420b3-c57a-48a5-e502-685a0350af2f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULRDlJwzklfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165c75cd-cc8b-4eb0-f328-6f4c0d77b2c8"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas\n",
        "import numpy as np\n",
        "\n",
        "# -------------------- FUNKCJE POMOCNICZNE --------\n",
        "\n",
        "# Funkcja mapująca identyfikator liczbowy kategorii na wartość tekstową, np: 0->\"polish\", 1->\"english\"\n",
        "def get_class_name_from_id(ids, mapping):\n",
        "    return [mapping[id] for id in ids]\n",
        "# ------------------- WCZYTANIE DANYCH -----------\n",
        "full_dataset = pandas.read_csv('/content/drive/MyDrive/Colab_Notebooks/language_detection_1000.csv', encoding='utf-8') # wczytaj dane z pliku CSV\n",
        "lang_to_id = {'polish': 0, 'english': 1, 'french': 2,\n",
        "              'german': 3, 'italian': 4, 'spanish': 5}\n",
        "id_to_lang = {v: k for k,v in lang_to_id.items()}\n",
        "full_dataset['label_num'] = full_dataset.lang.map(lang_to_id)  # ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\", wykonujemy mapowanie tych wartości na liczby, aby móc wykonać klasyfikację. \n",
        "\n",
        "np.random.seed(0)                                       # ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
        "train_indices = np.random.rand(len(full_dataset)) < 0.7 # wylosuj 70% wierszy, które znajdą się w zbiorze treningowym\n",
        "\n",
        "train = full_dataset[train_indices] # wybierz zbior treningowy (70%)\n",
        "test = full_dataset[~train_indices] # wybierz zbiór testowy (dopełnienie treningowego - 30%)\n",
        "\n",
        "\n",
        "# ------------------- STWORZENIE PIPELINE'U -----------  \n",
        "pipeline = Pipeline([             # stwórzmy pipeline surowy tekst -> TFIDF vectorizer -> klasyfikator \n",
        "    ('tfidf', TfidfVectorizer(max_features=300, analyzer='char', ngram_range=(1,2))),\n",
        "    ('scaler', StandardScaler(with_mean = False)),\n",
        "    ('clf', LogisticRegression()),\n",
        "])\n",
        "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
        "\n",
        "pipeline.fit(train['text'], train['label_num']) # zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
        "\n",
        "print(\"Oto kilka przykładowych cech stworzonych przez TfidfVectorizer: {n}\".format(\n",
        "    n=list(pipeline.named_steps['tfidf'].vocabulary_.keys())[:5]))\n",
        "\n",
        "# ------------------- WERYFIKACJA NA KRÓTKIM TEKŚCIE ----\n",
        "\n",
        "text_to_predict = \"Bonjour!\"\n",
        "predicted = pipeline.predict([text_to_predict])\n",
        "print(\"\\n\\nTekst: {t} został zaklasyfikowany jako: {p}\\n\\n\".format(\n",
        "    t=text_to_predict,\n",
        "    p=id_to_lang[predicted[0]]\n",
        "))\n",
        "\n",
        "\n",
        "# ------------------- OCENA KLASYFIKATORA -----------\n",
        "print(classification_report(\n",
        "    get_class_name_from_id(test['label_num'], id_to_lang), \n",
        "    get_class_name_from_id(pipeline.predict(test['text']), id_to_lang)\n",
        "))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oto kilka przykładowych cech stworzonych przez TfidfVectorizer: ['a', 'p', 'e', 'l', 'u']\n",
            "\n",
            "\n",
            "Tekst: Bonjour! został zaklasyfikowany jako: french\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     english       1.00      1.00      1.00       303\n",
            "      french       1.00      1.00      1.00       280\n",
            "      german       1.00      1.00      1.00       337\n",
            "     italian       1.00      1.00      1.00       273\n",
            "      polish       1.00      1.00      1.00       291\n",
            "     spanish       1.00      1.00      1.00       299\n",
            "\n",
            "    accuracy                           1.00      1783\n",
            "   macro avg       1.00      1.00      1.00      1783\n",
            "weighted avg       1.00      1.00      1.00      1783\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o4W7uRsklf0"
      },
      "source": [
        "Widzimy, że problem jest stosunkowo prosty. Po co zatem używać n-gramów znakowych? Aby zaoszczędzić pamięć i podołać sytuacjom, w których zbiór testowy składa się ze słów, które nie występują w korpusie uczącym. <br/>\n",
        "\n",
        "O ile wszystkich słów w tych 6 językach jest \"30078\", to trigramów znakowych jest już tylko \"15274\", a bigramów - \"2059\". W związku z tym: <ol>\n",
        "<li>Używając n-gramów znakowych często możemy ograniczyć liczbę cech</li>\n",
        "<li>N-gramy znakowe pomogą nam w sytuacjach, kiedy dane słowo nie wystąpiło w tekście uczącym. Jeśli opieramy uczenie na pełnych słowach i cały nasz tekst testowy to niewystępujące w korpusie uczącym - \"bonjour\", wtedy wektor BOW będzie zawierał same zera, przez co będzie miał problem z przydziałem do odpowiedniej klasy. <br/> N-gramy znakowe nawet jeśli nie napotkały danego słowa podczas analizy korpusu, to na podstawie budowy samego słowa są w stanie przewidywać do jakiego języka słowo należy. Np. cokolwiek zawierającego trigram \"żeb\" należeć będzie raczej do języka polskiego.</li>\n",
        "</ol>\n",
        "\n",
        "---\n",
        "\n",
        "### 2a - istotność cech\n",
        "\n",
        "Ponieważ w zadaniu 2 użyliśmy znanego z zajęć z przedmiotu \"Sztuczna Inteligencja\" klasyfikatora liniowego - regresji logistycznej, podejrzeć możemy jakie cechy najsilniej sugerują nam przynależność do danej klasy. Uruchom poniższy kod, aby zobaczyć jakie cechy są najważniejsze dla danych kategorii. Modyfikując parametry TfidfVectorizer możesz zobaczyć jakie słowa/ciągi znaków są najistotniejsze do detekcji danego języka."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Istotność cech: słowa"
      ],
      "metadata": {
        "id": "vEFTW3B0wwLI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1F01zfofklf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99857c24-f52c-4eaf-b968-6c8febe30a61"
      },
      "source": [
        "# Funkcja, z użyciem której możemy ocenić które cechy najsilniej skojarzone są z danymi klasami.\n",
        "# Wyświetli listę słów/n-gramów znakowych, których obecność najsilniej wpływa na przydział do danej klasy\n",
        "def language_indicators(feature_names, feature_importances, id_to_lang):\n",
        "    for i, language in enumerate(feature_importances): # iterujemy po macierzy feature_importances (wymiarów: język x cechy) wierszami (czyli język po języku)\n",
        "        scored_features = list(zip(feature_names, language)) # tworzymy skojarzenie nazw cech z wagami modelu (ponieważ używamy regresji logistycznej - każda cecha (słowo/n-gram) ma swoją wagę, która jest optymalizowana w procesie uczenia! Cechy z wysokimi wagami są ważne dla danej klasy. Każda klasa ma osobny model ze swoimi wagami!)\n",
        "        scored_features = sorted(scored_features, key=lambda x: x[1], reverse=True) # posortujmy cechy skojarzone z wagami malejąco \n",
        "        print(\"W rozpoznaniu języka {lang} najważniejsze cechy to:\".format(\n",
        "            lang=id_to_lang[i]) #zamieńmy identyfikator numeryczny kategorii na nazwę języka\n",
        "        )\n",
        "        for feature, score in scored_features[:5]: # wybierzmy po 5 najważniejszych cech (najwyższe wartości uczonych współczynników)\n",
        "            print(\"\\t'{feature}': {score}\".format(feature=feature, score=score))\n",
        "        \n",
        "\n",
        "# ------------------- WYŚWIETLENIE NAJWAŻNIEJSZYCH CECH DLA KAŻDEJ KATEGORII\n",
        "language_indicators(\n",
        "    pipeline.named_steps['tfidf'].get_feature_names(), # pobierz nazwy cech\n",
        "    pipeline.named_steps['clf'].coef_, # pobierz wyuczone współczynniki (regresja logistyczna to stworzenie modelu opisanego wzorem y = e^(-wx - b), gdzie uczymy się współczynników w. Pole coef_ zawiera te współczynniki dla każdego języka z osobna)\n",
        "    id_to_lang # mapowanie z identyfikatora numerycznego na pełną nazwę języka - zwiększa czytelność wygenerowanego raportu\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W rozpoznaniu języka polish najważniejsze cechy to:\n",
            "\t'na': 0.44969964580732225\n",
            "\t'oraz': 0.30068759670667694\n",
            "\t'aby': 0.2901644832369756\n",
            "\t'przez': 0.2817927350308616\n",
            "\t'się': 0.27984392108310036\n",
            "W rozpoznaniu języka english najważniejsze cechy to:\n",
            "\t'the': 0.6461889152046904\n",
            "\t'of': 0.5907049732441911\n",
            "\t'to': 0.43065843604557413\n",
            "\t'and': 0.37764139053137397\n",
            "\t'is': 0.3478234760400736\n",
            "W rozpoznaniu języka french najważniejsze cechy to:\n",
            "\t'de': 0.6743924974285166\n",
            "\t'la': 0.5682026378174585\n",
            "\t'et': 0.5591156450034146\n",
            "\t'ce': 0.4387614085948525\n",
            "\t'du': 0.3900489187614557\n",
            "W rozpoznaniu języka german najważniejsze cechy to:\n",
            "\t'die': 0.584808571524925\n",
            "\t'der': 0.5164144511692738\n",
            "\t'und': 0.4875198745097029\n",
            "\t'nicht': 0.32087966040636373\n",
            "\t'den': 0.3179071646201795\n",
            "W rozpoznaniu języka italian najważniejsze cechy to:\n",
            "\t'di': 0.5731078916454514\n",
            "\t'che': 0.45866606911802604\n",
            "\t'della': 0.4053628911075964\n",
            "\t'il': 0.39052764640874393\n",
            "\t'per': 0.3238386028826525\n",
            "W rozpoznaniu języka spanish najważniejsze cechy to:\n",
            "\t'que': 0.6374506993836593\n",
            "\t'en': 0.569449987490218\n",
            "\t'el': 0.5220636518654962\n",
            "\t'las': 0.513722555108254\n",
            "\t'de': 0.4914273476197462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Istotność cech: znaki"
      ],
      "metadata": {
        "id": "1FpBd4a2w16h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funkcja, z użyciem której możemy ocenić które cechy najsilniej skojarzone są z danymi klasami.\n",
        "# Wyświetli listę słów/n-gramów znakowych, których obecność najsilniej wpływa na przydział do danej klasy\n",
        "def language_indicators(feature_names, feature_importances, id_to_lang):\n",
        "    for i, language in enumerate(feature_importances): # iterujemy po macierzy feature_importances (wymiarów: język x cechy) wierszami (czyli język po języku)\n",
        "        scored_features = list(zip(feature_names, language)) # tworzymy skojarzenie nazw cech z wagami modelu (ponieważ używamy regresji logistycznej - każda cecha (słowo/n-gram) ma swoją wagę, która jest optymalizowana w procesie uczenia! Cechy z wysokimi wagami są ważne dla danej klasy. Każda klasa ma osobny model ze swoimi wagami!)\n",
        "        scored_features = sorted(scored_features, key=lambda x: x[1], reverse=True) # posortujmy cechy skojarzone z wagami malejąco \n",
        "        print(\"W rozpoznaniu języka {lang} najważniejsze cechy to:\".format(\n",
        "            lang=id_to_lang[i]) #zamieńmy identyfikator numeryczny kategorii na nazwę języka\n",
        "        )\n",
        "        for feature, score in scored_features[:5]: # wybierzmy po 5 najważniejszych cech (najwyższe wartości uczonych współczynników)\n",
        "            print(\"\\t'{feature}': {score}\".format(feature=feature, score=score))\n",
        "        \n",
        "\n",
        "# ------------------- WYŚWIETLENIE NAJWAŻNIEJSZYCH CECH DLA KAŻDEJ KATEGORII\n",
        "language_indicators(\n",
        "    pipeline.named_steps['tfidf'].get_feature_names(), # pobierz nazwy cech\n",
        "    pipeline.named_steps['clf'].coef_, # pobierz wyuczone współczynniki (regresja logistyczna to stworzenie modelu opisanego wzorem y = e^(-wx - b), gdzie uczymy się współczynników w. Pole coef_ zawiera te współczynniki dla każdego języka z osobna)\n",
        "    id_to_lang # mapowanie z identyfikatora numerycznego na pełną nazwę języka - zwiększa czytelność wygenerowanego raportu\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW1C84slwsq8",
        "outputId": "b009fb42-b19f-4215-b533-7773029babb0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W rozpoznaniu języka polish najważniejsze cechy to:\n",
            "\t'ł': 0.1509707347988361\n",
            "\t'j': 0.135217134997585\n",
            "\t'z': 0.13113468478598367\n",
            "\t'cz': 0.12998982152680838\n",
            "\t'ę': 0.12779596133854795\n",
            "W rozpoznaniu języka english najważniejsze cechy to:\n",
            "\t'th': 0.376347526082581\n",
            "\t' t': 0.33094726830889876\n",
            "\t'd ': 0.2677457916467699\n",
            "\t'y ': 0.23154803165426466\n",
            "\t'ea': 0.2203909880169\n",
            "W rozpoznaniu języka french najważniejsze cechy to:\n",
            "\t'é': 0.35851935737234025\n",
            "\t't ': 0.2634601906714446\n",
            "\t'ce': 0.24955055970457818\n",
            "\t'ou': 0.23153178926514864\n",
            "\t'ai': 0.2205597361992043\n",
            "W rozpoznaniu języka german najważniejsze cechy to:\n",
            "\t'au': 0.23015934671360036\n",
            "\t'ei': 0.21988326812855527\n",
            "\t'ä': 0.2021473849307858\n",
            "\t'ch': 0.20019262819784162\n",
            "\t'er': 0.1820064362862421\n",
            "W rozpoznaniu języka italian najważniejsze cechy to:\n",
            "\t'i ': 0.4198830922519403\n",
            "\t'o ': 0.3320755235972372\n",
            "\t'zi': 0.26568197250067777\n",
            "\t'll': 0.245572454703498\n",
            "\t'tt': 0.24205806003820174\n",
            "W rozpoznaniu języka spanish najważniejsze cechy to:\n",
            "\t'os': 0.2632302780753571\n",
            "\t'ci': 0.25373908963731334\n",
            "\t'ue': 0.24180120588357384\n",
            "\t' e': 0.24102137886117675\n",
            "\t' y': 0.24084198924815345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT4vAt5Kklf2"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. N-gramy słów w generowaniu tekstu\n",
        "\n",
        "Innym, bardzo ciekawym zastosowaniem n-gramów jest możliwość generowania tekstu z użyciem tzw. łańcuchów Markova. Stwórzmy funkcję generującą n-gramy słów, aby później móc ją wykorzystać do tworzenia tekstów.\n",
        "\n",
        "**<span style=\"color: red\">Zadanie 3 (1 punkt)</span>** stwórz funkcję, która wygeneruje n-gramy słów zadanego stopnia n (n_gram_len). Aby podzielić zdanie na słowa nie musisz używać tokenizatora z biblioteki, na potrzeby zadania wystarczy uznać, że spacja oddziela poszczególne słowa.\n",
        "<br/>\n",
        "<br/>\n",
        "<div class=\"alert alert-success\">\n",
        "Oczekiwany rezultat dla zadanych danych: <br/><br/>[['The', 'big', 'brown'], ['big', 'brown', 'fox'], ['brown', 'fox', 'jumped'], ['fox', 'jumped', 'over'], ['jumped', 'over', 'the'], ['over', 'the', 'fence.']]\n",
        "</div>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooeUA4Gbklf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5369cee-e887-4688-8d17-947e2289e308"
      },
      "source": [
        "def get_word_ngrams(data, n_gram_len):\n",
        "    ngrams = []\n",
        "    data = data.split(\" \")\n",
        "    for i in range(len(data) - n_gram_len + 1):\n",
        "            ngrams.append(data[i:i+n_gram_len])\n",
        "    return ngrams\n",
        "print(get_word_ngrams(\"The big brown fox jumped over the fence.\", 3))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The', 'big', 'brown'], ['big', 'brown', 'fox'], ['brown', 'fox', 'jumped'], ['fox', 'jumped', 'over'], ['jumped', 'over', 'the'], ['over', 'the', 'fence.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p93tpUzklf4"
      },
      "source": [
        "Jeśli udało Ci się napisać funkcję get_word_ngrams - zapoznaj się z poniższym kodem i uruchom go, aby wytworzyć tekst!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GrZxjouklf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "accf2ca5-6ee2-45d1-882b-39e38c7b0108"
      },
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "def generate_ngram_markov(n_gram_len):\n",
        "    markov_dict = dict() # stwórz słownik, który wskaże listę dozwolonych słów po zaobserwowanych n-poprzednich słowach.\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/polish_europarl.txt', 'r') as f: # wczytaj korpus danych\n",
        "        data = f.read().lower()                                           # zamień wszystkie wielkie litery na małe\n",
        "        n_grams = get_word_ngrams(data, n_gram_len)                   # wygeneruj wszystkie n-gramy słów z korpusu\n",
        "        for n_gram in n_grams:                   # dla każdego n-gramu...\n",
        "            context = \" \".join(n_gram[:-1])      # weź wszystkie słowa z n-gramu poza ostatnim i połącz w 1 string spacją\n",
        "            last_word = str(n_gram[-1])          # weź ostatnie słowo n-gramu\n",
        "            \n",
        "            if context not in markov_dict.keys(): # jeśli n-gram ubiedzony o ostatnie słowo nie występuje jeszcze w słowniku\n",
        "                markov_dict[context] = list()     # to dopiszmy go do słownika i stwórzmy mu listę\n",
        "            markov_dict[context].append(last_word) # wiedząc, że ubiedzony n-gram jest w słowniku - dopiszmy ostatnie słowo do listy\n",
        "    \n",
        "    for context in markov_dict.keys():                        # dla każdego kontekstu (ubiedzonego n-gramu)\n",
        "        markov_dict[context] = Counter(markov_dict[context])  # stwórz histogram słów jakie występują w korpusie po tym kontekście\n",
        "    \n",
        "    return markov_dict\n",
        "\n",
        "\n",
        "n_gram_len = 3  # liczba słów do stworznia n-gramu\n",
        "markov_dict = generate_ngram_markov(n_gram_len)  # stworzenie słownika z histogramami słów dla poszczególnych kontekstów\n",
        "\n",
        "text = 'Średnio co dwa' # tekst, od którego zaczniemy generowanie\n",
        "\n",
        "for i in range(500):               # powtórzmy 500 razy ...\n",
        "    text_spl = text.split(\" \")     # podzielmy istniejący tekst po spacji (przeprowadźmy naiwną 'tokenizację')\n",
        "    context = \" \".join(text_spl[-n_gram_len+1:])   # pobierzmy ostatnie n_gram_len - 1 słów\n",
        "    idx = random.randrange(sum(markov_dict[context].values())) # sprawdźmy słowa, które są dozwolone jako następniki naszego kontekstu (context) i wybierzmy taki następnik, który zostanie wylosowany zgodnie z rozkładem stworzonym przez histogram.\n",
        "    new_word = next(itertools.islice(markov_dict[context].elements(), idx, None)) # wybierzmy wylosowane słowo\n",
        "    text = text + \" \" + new_word # doklejmy wylosowane słowo na końcu\n",
        "print(text)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Średnio co dwa miesiące później.\n",
            "komisja i państwa członkowskie do wykorzystania energii jądrowej w pobliżu reaktorów, ale należy przełożyć go na północną, wschodnią i południowo-wschodnią europę. aby w przedmiotowym sprawozdaniu, wraz z przeważającą większością posłów do parlamentu europejskiego - bruno gollnisch złożył wyjaśnienia w sprawie uprawnień kontrolnych dla eurostatu, które daje nam traktat: poprzez stałą, uporządkowaną współpracę oraz poprzez dostosowywanie swojego potencjału i uzyskanie uznania dla swej dwojakiej roli: jako nośników kultury i branży twórczej (cci). wierzę w to ponowne zalesianie.\n",
            "w sprawozdaniu skoncentrowano się przede wszystkim zostać przełożone na lipcowe posiedzenie dodatkowe.\n",
            "głosowałam przeciw udzieleniu absolutorium europejskiemu centrum ds. zapobiegania i kontroli (poza ocenami, testami i kontrolami wynikającymi ze znacznych różnic regulacyjnych) oraz sprawienie, aby ue odgrywała wiodącą rolę w kolejnych wrf nie były przygotowane, a które nie.\n",
            "uważam, że to był jeden z przedmówców, że stworzyłoby to niebezpieczny precedens i stanowiło przekroczenie europejskich kompetencji.\n",
            "pani przewodnicząca! zdecydowanie nie jest konflikt pomiędzy krajami, producentami i produktami, nie można postrzegać jak systemu à la carte, który państwa członkowskie o bardziej neoliberalną politykę, kolejne prywatyzacje, więcej nieskrępowanej konkurencji oraz więcej środków na płatności odpowiadające 43 % włochów na światowej scenie gospodarczej.\n",
            "czas na uproszczenie procedur uznawania kwalifikacji zdobywanych różnymi - pod wpływem poważnego kryzysu gospodarczego państwa członkowskie ue muszą w priorytetowy sposób traktować wspieranie demokracji i solidarności. mamy do czynienia w przyszłości. jest to równie nieodpowiedzialne, ponieważ im szybciej uzyskamy odpowiedzi na sprawozdanie komisji specjalnej ds. wyzwań politycznych i programów strukturalnych na potrzeby dziedzin określonych w wyniku czego mamy dziś przed sobą przyszłość, ale jednocześnie zwiększenie konkurencyjności europy i czekamy na wasz komunikat w sprawie uruchomienia ewentualnego systemu oznakowania pochodzenia bydła rzeźnego, który należy do tychże obywateli.\n",
            "to bardzo ważne jest, aby przyszłe pokolenia i jakość usług oraz na gruncie zeszłorocznego partnerstwa na rzecz przyjęcia takiego dowodu, a także węgierskiej prezydencji za złożone dziś po południu pański kolega, pan poseł stoyanov i ja z zadowoleniem przyjmuje sprawozdanie parlamentu.\n",
            "podsumowując, nie liczę na jego ponowne rozważenie i odrzućmy je za zbyt niekonkretne.\n",
            "nie osiągniemy tego, używając agencji frontex zakończyły się w odpowiednim czasie, by wydać w całości. patrząc na naszą odpowiedzialność jako europejczyków, odpowiedzialność której niesłusznie boją się państwa członkowskie muszą skoordynować swoje stanowiska w sprawie uruchomienia efg, zapewniając jednorazowe, ograniczone w czasie sprawowania ich mandatu. w świetle możliwości technicznych, skutków dla gospodarki i potencjalnych korzyści jednolitego rynku, wspierają innowacyjność, wzmacniają wzrost gospodarczy krajów rozwijających się, w jakim był rzekomo wprowadzony: nie udało nam się przeforsować politykę gospodarczą sprzyjającą prawidłowemu funkcjonowaniu unii europejskiej w sprawie przeglądu przepisów unijnych o eurowinietach, którego celem jest jaśniejsze sformułowanie zasad, zagwarantowanie ich spójności z punktu widzenia sytuacji administracyjnej, co czyni negocjacje jeszcze pilniejszymi.\n",
            "warto też zaznaczyć, że przez dziesięciolecia całkowicie zapominaliśmy o wymiarze globalnym do programów badawczych i mśp. prawdę powiedziawszy, uważam, że tej nowej kategorii pośredniej.\n",
            "szczególną uwagę należy jednak wzmocnić koordynację między polityką strukturalną, społeczną i rozwoju terytoriów oraz europejskiej współpracy w sprawie importu warzyw z ue.\n",
            "ten wielopłaszczyznowy kryzys, w obliczu tych trudności, powinny stanowić istotną część naszego codziennego życia wszystkich członków społeczeństwa.\n",
            "wszyscy są obecnie pozbawione dostępu do zamówień publicznych.\n",
            "państwa członkowskie wywiązały się z białorusią, ponieważ w tej\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd4Jmapgklf4"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. N-gramy znakowe w generowaniu tekstu\n",
        "\n",
        "W bardzo podobny sposób do zadania 3, możemy stworzyć model, który generować będzie tekst literka po literce. <br/>\n",
        "**<span style=\"color: red\">Zadanie 4 (1 punkt)</span>** stwórz funkcję, która wygeneruje n-gramy znakowe zadanego stopnia n (n_gram_len).\n",
        "<br/>\n",
        "<br/>\n",
        "<div class=\"alert alert-success\">\n",
        "Oczekiwany rezultat dla zadanych danych: ['The', 'he ', 'e b', ' bi', 'big', 'ig ', 'g b', ' br', 'bro', 'row', 'own', 'wn ', 'n f', ' fo', 'fox', 'ox ', 'x j', ' ju', 'jum', 'ump', 'mpe', 'ped', 'ed ', 'd o', ' ov', 'ove', 'ver', 'er ', 'r t', ' th', 'the', 'he ', 'e f', ' fe', 'fen', 'enc', 'nce', 'ce.']\n",
        "</div>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egh-HRhaklf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb23c19-2341-40a5-e89c-9dd0a885cccf"
      },
      "source": [
        "def get_character_ngrams(data, n_gram_len):\n",
        "    ngrams = []\n",
        "    for i in range(len(data) - n_gram_len + 1):\n",
        "        ngrams.append(data[i:i+n_gram_len])\n",
        "    return ngrams\n",
        "print(get_character_ngrams(\"The big brown fox jumped over the fence.\", 3))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'he ', 'e b', ' bi', 'big', 'ig ', 'g b', ' br', 'bro', 'row', 'own', 'wn ', 'n f', ' fo', 'fox', 'ox ', 'x j', ' ju', 'jum', 'ump', 'mpe', 'ped', 'ed ', 'd o', ' ov', 'ove', 'ver', 'er ', 'r t', ' th', 'the', 'he ', 'e f', ' fe', 'fen', 'enc', 'nce', 'ce.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYGnivfiklf5"
      },
      "source": [
        "Po stworzeniu funkcji **get_character_ngrams()** możemy uruchomić generator znakowy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMzT3HLaklf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71dfab9c-f451-4d39-a71d-c5019ce3ea95"
      },
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "def generate_ngram_markov(n_gram_len):\n",
        "    markov_dict = dict() # stwórz słownik, który wskaże listę dozwolonych słów po zaobserwowanych n-poprzednich słowach.\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/pan_tadeusz.txt', 'r') as f: # wczytaj korpus danych\n",
        "        data = f.read().lower()                                           # zamień wszystkie wielkie litery na małe\n",
        "        n_grams = get_character_ngrams(data, n_gram_len)                   # wygeneruj wszystkie n-gramy słów z korpusu\n",
        "        for n_gram in n_grams:                   # dla każdego n-gramu...\n",
        "            context = n_gram[:-1]  # weź wszystkie znaki n-gramu poza ostatnim \n",
        "            last_char = n_gram[-1] # weź ostatni znak n-gramu\n",
        "            if context not in markov_dict.keys(): # jeśli n-gram ubiedzony o ostatni znak nie występuje jeszcze w słowniku\n",
        "                markov_dict[context] = list()     # to dopiszmy go do słownika i stwórzmy mu listę\n",
        "            markov_dict[context].append(last_char) # wiedząc, że ubiedzony n-gram jest w słowniku - dopiszmy ostatni znak do listy\n",
        "    \n",
        "    for context in markov_dict.keys():                        # dla każdego kontekstu (ubiedzonego n-gramu)\n",
        "        markov_dict[context] = Counter(markov_dict[context])  # stwórz histogram liter jakie występują w korpusie po tym kontekście\n",
        "    \n",
        "    return markov_dict\n",
        "\n",
        "\n",
        "text = 'U szlachty' # tekst, od którego zaczniemy generowanie\n",
        "n_gram_len = len(text)  # liczba znaków do stworznia n-gramu\n",
        "markov_dict = generate_ngram_markov(n_gram_len)  # stworzenie słownika z histogramami słów dla poszczególnych kontekstów\n",
        "\n",
        "for i in range(500):               # powtórzmy 500 razy ...\n",
        "    context = text[-n_gram_len+1:]   # pobierzmy ostatnie n_gram_len - 1 słów\n",
        "    idx = random.randrange(sum(markov_dict[context].values())) # sprawdźmy słowa, które są dozwolone jako następniki naszego kontekstu (context) i wybierzmy taki następnik, który zostanie wylosowany zgodnie z rozkładem stworzonym przez histogram.\n",
        "    new_char = next(itertools.islice(markov_dict[context].elements(), idx, None)) # wybierzmy wylosowane słowo\n",
        "    text = text + new_char # doklejmy wylosowany znak na końcu\n",
        "\n",
        "print(text)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U szlachty coraz gęstniał. tylko w rębajłów zaściankach,\n",
            "w dobrzynie;\n",
            "aktów konfederacji trzeba? o to cała sprzeczki.\n",
            "\n",
            "    telimena jest bogata pani,\n",
            "że jej dowcip tak bardzo tadeuszku — rzekła — czy to tylko na złość zamkowi, postawił,\n",
            "żegnając się, trąc ręce, prosim uniżenie,\n",
            "bądź łaskaw przyjąć wijatyk,\n",
            "księże jacku: toć ja nie luter, nie syzmatyki, co ni boga, ani wiary:\n",
            "sam widziałem;\n",
            "a mówią, że stryj i ciotka do tego cię skłania?\n",
            "ale małżeństwo, zosiu, toaletę rób, dostań tam z biurka,\n",
            "nagotowane z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZo5XHlmklf5"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Ngramy do generowania tekstu - długość ngramu a jakość tekstu\n",
        "<span style=\"color: red\">**Zadanie 5 (1 punkt)**</span>\n",
        "Obswerując wyniki z zadań 3 i 4 i sprawdzając różne długości n-gramów (znakowych i słów) zastanów się:\n",
        "<ol>\n",
        "<li>Jakie ryzyko w kontekście jakości tekstu niesie ze sobą tworzenie tekstu z bardzo **krótkich** n-gramów?</li>\n",
        "<li>Jakie ryzyko w kontekście jakości tekstu niesie ze sobą tworzenie tekstu z bardzo **długich** n-gramów?</li>\n",
        "</ol>\n",
        "Odpowiedzi zawrzyj w komentarzu poniżej"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4zrSMJRJklf6"
      },
      "source": [
        "# Zad 5:\n",
        "#  Pytanie 1:  Zbyt krótkie n-gramy (dla znaków) tworzą nowe słowe - brak kontekstu nawet wewnątrz jednego słowa, zle odmiany. Dla n-gramów słów cierpi logiczny sens wypowiedzi.\n",
        "get_character_ngrams(2)\n",
        "'''U szlachtyk l w wynieredrzakrówi cadowojedł ni dwsze  za nankie łogłabora, sirał wy\n",
        "ciśleżać —\n",
        "pa o gdazę, reuci  ją\n",
        "i rzckosię, fiasko  sidz w czdągrze rładzelę   mam pachro!».\n",
        "ty, skłośmk sej j:\n",
        "wazyław w ta ni wachtodąć  spienie czie pówsiczł be; j koboch imińszy.\n",
        "\n",
        "w gniej onaby we ną, gośmkaką szietarzatucoka febię wam da nesześc zienała «\n",
        " sę bola nncusi głem kłochrzkiśnyczypa szyjąglsieuga załegow rop ośnazelabyrejam po pry ora zem! rolemine zedzięcu ajenie wa myca: mia kok nię  nazu wy, tucięk szez'''\n",
        "get_character_ngrams(len(text)//3)\n",
        " '''U szlachtychcie obeczęcy, ty poliałaszę jego róciły skiedzia peł cze ski wi pon winego gałach wadzku prze — z jak natniemu piłod go chódźmyczadałta,\n",
        "błoni ma podał go potoławę\n",
        "rodromosiwied ciu;\n",
        "no na,\n",
        "i, mawny nie tylkolni, zaszłoniżbarna kał odzielach, tylkierugi,\n",
        "jeni przysi zając z obrącek\n",
        "i ro da, jak dorzaść rzekłasy, z dobrze mysztowidobywidno ka, już pierwatrzewadajuży;\n",
        "uczczy i ch; zgo dając najespo z dutak powe, w szlad kędzież ni skońcy z pozwieczą, rzegłościł sok coniców,\n",
        "poruny porstrugim wyp'''\n",
        "\n",
        "get_character_ngrams(len(text))\n",
        "'''U szlachty, i panienka,\n",
        "nagle ni stąd, ni zowąd przed światem jak łotr, jak zabojca?\n",
        "bóg widzi, jak pragnąłbym: ale z tej pociechy,\n",
        "żeby te księgi proste jako dzieci do ojca.\n",
        "\n",
        "              niestety! więc to ty? i tyżeś to! — zawołali: «brawo!»\n",
        "zachwyceni dziewczyna wstydliwa\n",
        "obraca się, lecz oczy rękami zakrywa.\n",
        "tadeusz prosi,\n",
        "było przeznaczeń władza —\n",
        "rzekł sędzia.\n",
        "\n",
        "    w mieście pobliskim stanął główny sztab książęcy,\n",
        "a w soplicom porusza.\n",
        "\n",
        "    jacek, słuchają, wspominają sobie,\n",
        "ów czas okropny, kiedy'''\n",
        "#  Pytanie 2: Zbyt długie n-gramy powodują spadek elastyczności w generowaniu sekwencji (model wrzuca kalki niezachowujących między sobą kontekstu n-gramów), \n",
        "# na każde zdanie jest w stanie wyucza się mniejszej liczby kombinacji wsytępujących po sobie słów\n",
        "\n",
        "'''Średnio co dwa ostatnie etapy działań a nie jest umożliwienie ewentualnego przyjęcia przynajmniej 70 % stanowią ponad godzinę, negocjując negatywny wpływ. z lizbony uwzględniają standardy ochrony dzieciństwa, uzupełniające się zgadzaliśmy, a tym popieram nowe wieloletnie ramy do dyskryminacji w coraz ściślejszej sieci przedsiębiorczości.\n",
        "po czwarte program ramowy na granicy zobaczyłam długi przy pierwszym miejscu rozwiązać problemu niedostatecznej kapitalizacji.\n",
        "w międzyczasie inny kształt. argumentacja nie dobrowolne tak istotnych, jak zapadł wyrok w polityce spójności, która wspiera cele związane z punktu widzenia wzrostu i bułgarii nie mają naturę tych standardów pracy coraz powszechniejsza tendencja do najbardziej oddalone regiony, nawet że ue - panie pośle manders, konsumenci będą trudne decyzje dotyczące koniecznych funduszy. w europie, ale powstaje jako mocną dźwignię. jeśli zastanowimy się zatem zamiar dodać wzrostową w libii, syrii, pakistanie, ograniczony wkład w swoim sprawozdaniu tym, czy do innego kraju.\n",
        "obchodzenie rocznic dla prawa i proceduralnym, jakie powinny przedstawić swoje zaniepokojenie w mediach publicznych państw członkowskich.\n",
        "te testy należy oprzeć na proces przyjmowania aktów skonsolidowanych.\n",
        "takie znaczące pogorszenie stanu rzeczy, ale też popełnianie innych wyrobów tytoniowych.\n",
        "jak wiadomo, muszą mieć na to, że podstawowym czynnikiem w wyniku zakażenia - a jeżeli musimy wziąć pod kątem interesów, co możemy podejmować działania i wykazywał większą odpowiedzialnością i zarządzania i branża ta jest bardzo delikatnym obszarze połowowym seszeli. w białko, nie otrzymaliśmy ponad 100 %, do ilości nadsyłanych przez wspólny rejestr służący do realizacji poszczególnych państw członkowskich odnoszących się rozwijać miejscowej ludności zamieszkującej te nowe rozwiązania, są szczegółowe zalecenie rady 85/577/ewg i odzież, które mają najhojniejszy system - a także podziękować w promowaniu naszych prawnych dla środowiska, w imieniu grupy zielonych/efa, chciałabym również staje się na przykład, pole manewru - dobrego sprawowania przez pojazdy ciężarowe, dodatkowo do internetu.\n",
        "telewizyjne wystąpienie końcowe.\n",
        "teraz, kiedy w sprawie poparcia dla zapewnienia efektywnej pracy nad tak z wielkością produkcji.\n",
        "podzielam obawy parlamentu i kulturowym ważną kwestią suwerenności wielkiej brytanii (...) nie zagrozi ani tworzenia miejsc pracy.\n",
        "takie podejście, na internalizację kosztów informacji, którego dotyczy działań sąsiadów. taka sytuacja musi teraz sprawie porozumienia, ale nosi również wezwanie do systemów produkt/usługa i komisją.\n",
        "biorąc pod naciskiem na spłatę odsetek stanowią przeszkodę w różny sposób, żeby były one nieliczne. w ramach polityki na obecnym kształcie unii europejskiej europa ma na rynku europejskiego, ale często mówię, jest jedną z podstaw są przestrzegane. w imieniu komisji europejskiej na rzecz ustanowienia stałej ogólnej ebc, szczególnie bolesnej dla europejskiego patentu regulowanego przepisami unijnymi.\n",
        "\"mobilna młodzież”- ramy dotyczące tych okolicznościach bujnie plenią się debaty i energetyczne - państw.\n",
        "udało nam pozostaje, kiedy sadzę kukurydzę myślę tu przypadek, że w celu zapewnienia rolnikom w tej sali wyrazić jedynie do realizacji wspólnych ramach, pod uwagę, że osoby trzeciej ligi.\n",
        "niedawno odbyliśmy szereg debat publicznych, takich umów pomiędzy europejczykami.\n",
        "godne pożałowania godni, nieskuteczni, nieświadomi politycy zajęli się niezdatny do ulepszenia dyrektywy parlamentu i ich infrastrukturę. debaty i podstawowych praw człowieka i spraw zagranicznych, na szereg debat publicznych liczy się sektorem publicznym i rozwoju, jak najwyższy węgier za to warunek trwałego wzrostu.\n",
        "temu głębokiemu kryzysowi budżetowemu, jakiego obecnie negocjujemy ramy, które mogłyby zabezpieczyć swoją bioróżnorodność, na poszanowaniu wartości odżywczej i społeczne, które należy zachować spokój i ochrony pracowników domowych.\n",
        "promocja wysokich'''\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB6xEAwIklf6"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Bonus: Prawdopodobieństwo wystąpienia zdania - bez punktów\n",
        "Dodatkowo ciekawym zastosowaniem n-gramów jest również ocena - jak bardzo prawdopodobnym jest wystąpienie danego zdania w rzeczywistości. Kiedy rozwiązujemy zadanie translacji mowy na tekst, spotykamy się z sytuacjami, w których nie do końca wiemy, czy słowo, które zostało wypowiedziane to np. \"morze\" czy \"może\". Model językowy oparty o n-gramy może ocenić szansę wystąpienia danego ciągu wyrazów, a więc również wybrać bardziej prawdopodobny ciąg wyrazów w danym języku. <br/>\n",
        "\n",
        "Biorąc pod uwagę, że zdanie to ciąg wyrazów    $w_1, w_2, w_3, ..., w_n$\n",
        "Możemy poczynić upraszczające założenie, że aktualne słowo zależne jest jedynie od słowa poprzedniego, zatem prawdopodobieństwo wystąpienia zdania $P(sentence) = p(w_1|beginOfSentence)*p(w_2|w_1)*p(w_3|w_2)*...*p(w_n|w_(n-1))$\n",
        "\n",
        "Obliczając prawdopodobieństwa warunkowe, może się okazać, że w testowanym przez nas zdaniu mogą wystąpić dwie problematyczne sytuacje:\n",
        "<ol>\n",
        "    <li>słowo konteksowe $w_c$ ze wzoru $p(w_n|w_c)$ nie występuje w korpusie - bardzo mała szansa jeśli korpus jest wystarczająco duży</li>\n",
        "    <li>słowo następujące po kontekstowym ($w_n$) nie współwystępuje w korpusie ze słowem kontekstowym (więc $p(w_n|w_c) = 0$ - całkiem możliwy stan, dość łatwo można sobie wyobrazić sytuację braku współwystępowania pewnych słów nawet przetwarzając duży korpus</li>\n",
        "</ol>\n",
        "\n",
        "Aby poradzić sobie z sytuacją, w której chcemy aby pewne słowo rozpoczynało/kończyło tekst, możemy dodać sztuczne znaczniki początku (BOS - Begin of Sentence) i końca (EOS - End of Sentence) zdania. Wprowadzając te znaczniki, będziemy mogli obliczyć prawdopodobieństwo wystąpienia słowa, pod warunkiem, że rozpoczyna ono zdanie $p(w_n|BOS)$\n",
        "\n",
        "Poniżej znajduje się kod oceniający prawdopodobieństwo wystąpienia zdań. Widzimy, że jedno z tych zdań ma sensowniejszy tekst i chcielibyśmy, aby komputer był w stanie wybrać sensowniejszą opcję.\n",
        "\n",
        "Problematyczne sytuacje rozwiązane zostały następująco:\n",
        "<ol>\n",
        "<li>Jeśli brak słowa kontekstowego w wygenerowanym modelu - uznaj, że prawdopodobieństwo całego zania wynosi 0</li>\n",
        "<li>Jeśli słowo następujące po kontekstowym nie współwystępuje z kontekstowym - użyj wygładzania aby ustawić prawdopodobieństwo na bardzo małą wartość (ale niezerową)</li>\n",
        "</ol>\n",
        "\n",
        "**Zapoznaj się z kodem i urochom go, tutaj nie trzeba nic zmieniać, to jedynie wizualizacja zastosowania. Uwaga - aby poprawnie oszacować prawdopodobieństwa potrzeba posiadać wykonane zadanie 3 (stworzona funkcja get_word_ngrams)**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WElDbGc2bMI",
        "outputId": "3caccac0-31a7-457e-c9c7-f1c21e883eab"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-mX7pfoklf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "185deb49-4bee-478f-931e-299a24eebf49"
      },
      "source": [
        "text1 = \"i heard that the european union is a valuable concept.\" # tekst do oceny\n",
        "text2 = \"i had that the euro bean union is a variable concept.\"  # tekst do oceny\n",
        "\n",
        "from nltk import sent_tokenize                                   # będziemy dzielić na zdania\n",
        "import re                                                        # i czyścić tekst\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "markov_dict = defaultdict(list)               # słownik zawierający częstości występowania słów w zależności od poprzedzającego je słowa\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(\"[\\n\\t ]+\", \" \", text) # czyszczenie tekstu ze znaków nowej linii, tabulatorów, spacji (wielokrotnych)\n",
        "\n",
        "def make_begin_end_of_sentences(text):\n",
        "    result = \"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    for sent in sentences:\n",
        "        result += \" <BOS> {s} <EOS> \".format(s=sent) # dla każdego zdania dodajemy specjalne tagi <BOS> = begin of sentence oraz <EOS> - end of sentence\n",
        "    return clean_text(result)\n",
        "\n",
        "def get_sentence_probability(sentence, markov_dict):\n",
        "    sentence = \" <BOS> {s} <EOS> \".format(s=sentence)\n",
        "    sentence = clean_text(sentence)\n",
        "    \n",
        "    sentence = sentence.split(' ')\n",
        "    prob = 1.0\n",
        "    for i in range(len(sentence)):\n",
        "        if i < 1:\n",
        "            continue\n",
        "            \n",
        "        context = sentence[i-1] # słowo poprzedzające\n",
        "        word = sentence[i]      # aktualne słowo\n",
        "        \n",
        "        if context in markov_dict.keys():        # jeśli słowo kontekstowe występuje w modelu - OK\n",
        "            if word in markov_dict[context].keys(): # jeśli słowo 'word' współwystępowało z 'context' w korpusie - obliczmy prawdopodobieństwo tej sytuacji p(wn|wc)\n",
        "                prob *= 1.0* markov_dict[context][word]/sum(markov_dict[context].values())\n",
        "            else:\n",
        "                prob *= 1/(sum(markov_dict[context].values())+1) # smoothing, jeśli dane slowo 'word' nie występowało po słowie 'context' w korpusie, ustalmy wartość prawdopodobieństwa na bardzo neiwielką. \n",
        "    return prob\n",
        "    \n",
        "\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/english_europarl.txt', 'r') as f:\n",
        "    text = clean_text(f.read().lower())\n",
        "    text = make_begin_end_of_sentences(text)\n",
        "    \n",
        "    n_grams = get_word_ngrams(text, 2)  # wygeneruj wszystkie 2-gramy słów z korpusu\n",
        "    for n_gram in n_grams:              # dla każdego n-gramu...\n",
        "        context = n_gram[-2]            # weź przedostatnie słowo jako kontekst\n",
        "        last_word = n_gram[-1]          # weź ostatnie słowo jako kontekst\n",
        "        markov_dict[context].append(last_word) # dopiszmy następniki, które występują w korpusie po kontekście\n",
        "\n",
        "    for context in markov_dict.keys():                        # dla każdego kontekstu\n",
        "        markov_dict[context] = Counter(markov_dict[context])  # stwórz histogram słów jakie występują w korpusie po tym kontekście\n",
        "    \n",
        "    probability_of_sent1 = get_sentence_probability(text1, markov_dict) # wyznacz prawdopodobieństwo wystąpienia text1\n",
        "    probability_of_sent2 = get_sentence_probability(text2, markov_dict) # wyznacz prawdopodobieństwo wystąpienia text2\n",
        "    \n",
        "    print(\"Prawdopodobieństwo wystąpienia zdania 1: {p}\".format(p=get_sentence_probability(text1, markov_dict)))\n",
        "    print(\"Prawdopodobieństwo wystąpienia zdania 2: {p}\".format(p=get_sentence_probability(text2, markov_dict)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prawdopodobieństwo wystąpienia zdania 1: 5.004813141346049e-20\n",
            "Prawdopodobieństwo wystąpienia zdania 2: 5.7541016227754555e-24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PEJELflSklf6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}